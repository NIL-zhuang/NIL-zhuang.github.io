<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Spark集群搭建 - 抓鱼 zzy's Blog</title><meta name=Description content="ZZY的博客"><meta property="og:title" content="Spark集群搭建"><meta property="og:description" content="写在前面 本文档是俩叫nil和mark的靓仔和yyds李传艺老师，在yyds的办公室倒腾一整个下午之后，总结出的血泪教训。 本文是在manjar"><meta property="og:type" content="article"><meta property="og:url" content="https://nil-zhuang.github.io/2022/10/spark_build/"><meta property="og:image" content="https://nil-zhuang.github.io/logo.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-10-28T15:12:56+08:00"><meta property="article:modified_time" content="2022-10-28T15:17:18+08:00"><meta property="og:site_name" content="抓鱼"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://nil-zhuang.github.io/logo.png"><meta name=twitter:title content="Spark集群搭建"><meta name=twitter:description content="写在前面 本文档是俩叫nil和mark的靓仔和yyds李传艺老师，在yyds的办公室倒腾一整个下午之后，总结出的血泪教训。 本文是在manjar"><meta name=application-name content="ZZY's Blog"><meta name=apple-mobile-web-app-title content="ZZY's Blog"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel=icon href=icon.svg><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://nil-zhuang.github.io/2022/10/spark_build/><link rel=prev href=https://nil-zhuang.github.io/2022/10/git/><link rel=next href=https://nil-zhuang.github.io/2022/10/hugo/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Spark集群搭建","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/nil-zhuang.github.io\/2022\/10\/spark_build\/"},"genre":"posts","wordcount":2823,"url":"https:\/\/nil-zhuang.github.io\/2022\/10\/spark_build\/","datePublished":"2022-10-28T15:12:56+08:00","dateModified":"2022-10-28T15:17:18+08:00","license":"Ziyuan Zhuang 2022","publisher":{"@type":"Organization","name":"Ziyuan Zhuang","logo":{"@type":"ImageObject","url":"https:\/\/nil-zhuang.github.io\/images\/avatar.jpeg","width":820,"height":820}},"author":{"@type":"Person","name":"Nil Zhuang"},"description":""}</script></head><body data-header-desktop=auto data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="抓鱼 zzy's Blog">🦊 Catch the Fish</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>Posts </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/categories/>Categories </a><a class=menu-item href=/about/>About </a><a class=menu-item href=https://github.com/NIL-zhuang rel="noopener noreffer" target=_blank>🦊 </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span>
</span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="抓鱼 zzy's Blog">🦊 Catch the Fish</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/posts/ title>Posts</a><a class=menu-item href=/tags/ title>Tags</a><a class=menu-item href=/categories/ title>Categories</a><a class=menu-item href=/about/ title>About</a><a class=menu-item href=https://github.com/NIL-zhuang title rel="noopener noreffer" target=_blank>🦊</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Spark集群搭建</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://github.com/NIL-zhuang title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>Nil Zhuang</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2022-10-28>2022-10-28</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;2823 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;6 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept=true><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#写在前面>写在前面</a></li><li><a href=#操作流程>操作流程</a></li><li><a href=#搭建ssh环境>搭建ssh环境</a></li><li><a href=#修改hosts文件>修改hosts文件</a></li><li><a href=#安装java环境>安装java环境</a></li><li><a href=#安装hadoop>安装hadoop</a><ul><li><a href=#修改配置文件>修改配置文件</a></li><li><a href=#验证hadoop集群安装成功>验证hadoop集群安装成功</a></li><li><a href=#验证hadoop集群运行状态>验证hadoop集群运行状态</a></li></ul></li><li><a href=#安装spark>安装spark</a></li><li><a href=#启动集群运行命令>启动集群，运行命令</a></li></ul></nav></div></div><div class=content id=content><h2 id=写在前面>写在前面</h2><p>本文档是俩叫nil和mark的靓仔和yyds李传艺老师，在yyds的办公室倒腾一整个下午之后，总结出的血泪教训。</p><p>本文是在manjaro和Ubuntu两个Linux发行版的个人PC上，启动hadoop hdfs集群和spark集群的操作过程。涉及到不同<strong>username</strong>和不同<strong>hostname</strong>的物理机，在真实网络情况下(宿舍路由器局域网)的配置操作，仅供参考。</p><p>其中DataNode机器的配置如下（夹带私货，欢迎加入manjaro邪教）。本文档所有命令都可以在manjaro/ArchLinux下执行，Ubuntu或其他系统请自行寻找对照命令</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-txt data-lang=txt><span class=line><span class=cl>OS: Manjaro 20.1.1 Mikah
</span></span><span class=line><span class=cl>Kernel: x86_64 Linux 5.8.11-1-MANJARO
</span></span><span class=line><span class=cl>Shell: zsh 5.8
</span></span><span class=line><span class=cl>CPU: Intel Core i7-8550U @ 8x 4GHz
</span></span><span class=line><span class=cl>Disk: 196G / 482G (42%)
</span></span><span class=line><span class=cl>GPU: Mesa Intel(R) UHD Graphics 620 (KBL GT2)
</span></span><span class=line><span class=cl>RAM: 5227MiB / 15896MiB
</span></span></code></pre></td></tr></table></div></div><table><thead><tr><th style=text-align:center>机器类型</th><th style=text-align:center>局域网IP</th><th style=text-align:center>hostname</th><th style=text-align:center>username</th></tr></thead><tbody><tr><td style=text-align:center>master/slaver</td><td style=text-align:center>192.168.1.103</td><td style=text-align:center>mark-pc</td><td style=text-align:center>mark</td></tr><tr><td style=text-align:center>slaver</td><td style=text-align:center>192.168.1.104</td><td style=text-align:center>nil-PC</td><td style=text-align:center>nil</td></tr></tbody></table><p>集群环境是</p><ul><li>Hadoop 2.7.7</li><li>Spark version 3.0.1</li><li>Scala version 2.12.10</li><li>OpenJDK 64-Bit Server VM, Java 1.8.0_265</li></ul><h2 id=操作流程>操作流程</h2><ol><li>创建镜像 or 还原点 (可选，推荐)</li><li>搭建ssh环境</li><li>修改hosts文件</li><li>安装java环境</li><li>安装hadoop</li><li>安装spark</li><li>启动集群，运行命令</li><li>开汽水庆祝</li></ol><h2 id=搭建ssh环境>搭建ssh环境</h2><p>首先配置好自己的ssh环境</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>ssh-keygen -t rsa
</span></span><span class=line><span class=cl>chmod <span class=m>600</span> ~/.ssh/authorized_keys
</span></span><span class=line><span class=cl>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-txt data-lang=txt><span class=line><span class=cl>文件目录如下
</span></span><span class=line><span class=cl>.ssh
</span></span><span class=line><span class=cl>├── authorized_keys
</span></span><span class=line><span class=cl>├── config
</span></span><span class=line><span class=cl>├── id_rsa
</span></span><span class=line><span class=cl>├── id_rsa.pub
</span></span><span class=line><span class=cl>└── known_hosts
</span></span></code></pre></td></tr></table></div></div><p>用<code>ifconfig</code>或<code>ip address show</code>查看ip地址后，ping一下你要连接的电脑的ip，保证网络通畅。</p><ul><li>免密登录： 将其他机器的id_rsa.pub里的内容添加到本机的authorized_keys里(用<code>cat >></code>追加)</li></ul><p>测试ssh连接。如果出现<code>ssh：connect to host port 22： Connection refused</code>，那记得运行命令<code>systemctl start sshd</code>打开sshd.sevice。</p><p>如果我在nil-PC上连接mark-pc，直接<code>ssh 192.168.1.103</code>会失败，因为的登录用户名是nil，而mark-pc里没有nil这个用户。正确做法是<code>ssh mark@192.168.1.103</code>，接收对方fingerprint到known_hosts里后，就可以免密登录。</p><h2 id=修改hosts文件>修改hosts文件</h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-txt data-lang=txt><span class=line><span class=cl>这是个栗子
</span></span><span class=line><span class=cl># Host addresses
</span></span><span class=line><span class=cl># 127.0.0.1  localhost
</span></span><span class=line><span class=cl># 127.0.1.1  nil-PC
</span></span><span class=line><span class=cl>::1        localhost ip6-localhost ip6-loopback
</span></span><span class=line><span class=cl>ff02::1    ip6-allnodes
</span></span><span class=line><span class=cl>ff02::2    ip6-allrouters
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>192.168.1.103 mark-pc
</span></span><span class=line><span class=cl>192.168.1.104 nil-PC
</span></span></code></pre></td></tr></table></div></div><p>Notice：</p><ol><li>将127.0.0.1和127.0.1.1的路径都去掉，否则在hadoop启动的时候，会优先在localhost提交出错</li><li>在ip后面，要写的是<strong>hostname</strong>，可以执行<code>hostname</code>看到，<strong>不是username</strong></li><li>在修改完后，一定要刷新hosts，运行命令<code>systemctl restart NetworkManager</code>或是注销重启</li></ol><p>修改后检查：</p><ol><li>ping mark-pc成功</li><li>ssh mark@mark-pc可以登录</li></ol><h2 id=安装java环境>安装java环境</h2><p>正常安装java8，java home不出意外在<code>/usr/lib/jvm/</code>下，本机路径是<code>/usr/lib/jvm/java-8-openjdk/</code>，可以在bin目录下执行<code>./javac -version</code>看到java版本是javac 1.8.0_265(好像1.8.0后的小小版本不影响，mark是261也运行正常)</p><p>将java添加至环境变量，在/etc/profile文件追加如下内容(具体内容因人而异)</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=nb>export</span> <span class=nv>JAVA_HOME</span><span class=o>=</span>/usr/lib/jvm/java-8-openjdk
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>CLASSPATH</span><span class=o>=</span>:<span class=nv>$JAVA_HOME</span>/lib:<span class=nv>$JAVA_HOME</span>/jre/lib:<span class=nv>$CLASSPATH</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>PATH</span><span class=o>=</span><span class=nv>$JAVA_HOME</span>/bin:<span class=nv>$JAVA_HOME</span>/jre/bin:<span class=nv>$PATH</span>
</span></span></code></pre></td></tr></table></div></div><p>每一次修改后都要<code>source /etc/profile</code>刷新环境变量。在命令行输入<code>echo $JAVA_HOME</code>进行检验</p><h2 id=安装hadoop>安装hadoop</h2><ol><li>选择了hadoop-2.7.7，也可以选最新文件。我将其解压在了/usr/lib/hadoop-2.7.7下（这个随便，但是不建议放在/home里面，涉及到用户权限）</li></ol><p>notice:<strong>所有机器上，hadoop和spark的绝对位置和内容要保持严格一致</strong></p><ol start=2><li>将hadoop添加到环境变量，追加了（具体内容因人而异）</li></ol><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=nb>export</span> <span class=nv>HADOOP_HOME</span><span class=o>=</span>/usr/lib/hadoop-2.7.7
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>PATH</span><span class=o>=</span><span class=nv>$PATH</span>:<span class=nv>$HADOOP_HOME</span>/bin:<span class=nv>$HADOOP_HOME</span>/sbin
</span></span></code></pre></td></tr></table></div></div><ol start=3><li>验证安装，执行<code>hadoop version</code></li></ol><h3 id=修改配置文件>修改配置文件</h3><p>注意，这里的<strong>mark-pc都要改成期望作为master的电脑的hostname</strong>，<strong>绝对路径也要做相应的修改</strong></p><ol><li><p>在$HADOOP_HOME下添加目录tmp, hdfs, hdfs/name, hdfs/data</p></li><li><p>$HADOOP_HOME/etc/hadoop/core-site.xml</p></li></ol><p>注意，在集群模式下，<strong>fs.defaultFS要写集群里namenode的ip和端口</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-xml data-lang=xml><span class=line><span class=cl><span class=nt>&lt;configuration&gt;</span>
</span></span><span class=line><span class=cl>  <span class=c>&lt;!-- 指定namenode的地址 --&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nt>&lt;property&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nt>&lt;name&gt;</span>fs.defaultFS<span class=nt>&lt;/name&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nt>&lt;value&gt;</span>hdfs://mark-pc:9000<span class=nt>&lt;/value&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nt>&lt;/property&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nt>&lt;property&gt;</span>
</span></span><span class=line><span class=cl>    <span class=c>&lt;!-- 用来指定使用hadoop时产生文件的存放目录 --&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nt>&lt;name&gt;</span>hadoop.tmp.dir<span class=nt>&lt;/name&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nt>&lt;value&gt;</span>/usr/lib/hadoop-2.7.7/tmp<span class=nt>&lt;/value&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nt>&lt;/property&gt;</span>
</span></span><span class=line><span class=cl><span class=nt>&lt;/configuration&gt;</span>
</span></span></code></pre></td></tr></table></div></div><ol start=3><li>$HADOOP_HOME/etc/hadoop/hdfs-site.xml</li></ol><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-xml data-lang=xml><span class=line><span class=cl><span class=nt>&lt;configuration&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nt>&lt;property&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nt>&lt;name&gt;</span>dfs.namenode.secondary.http-address<span class=nt>&lt;/name&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nt>&lt;value&gt;</span>mark-pc:50090<span class=nt>&lt;/value&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nt>&lt;/property&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nt>&lt;property&gt;</span>
</span></span><span class=line><span class=cl>    <span class=c>&lt;!-- 指定hdfs保存数据的副本数量 --&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nt>&lt;name&gt;</span>dfs.replication<span class=nt>&lt;/name&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nt>&lt;value&gt;</span>1<span class=nt>&lt;/value&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nt>&lt;/property&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nt>&lt;property&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nt>&lt;name&gt;</span>dfs.namenode.name.dir<span class=nt>&lt;/name&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nt>&lt;value&gt;</span>file:/usr/lib/hadoop-2.7.7/hdfs/name<span class=nt>&lt;/value&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nt>&lt;/property&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nt>&lt;property&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nt>&lt;name&gt;</span>dfs.datanode.data.dir<span class=nt>&lt;/name&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nt>&lt;value&gt;</span>file:/usr/lib/hadoop-2.7.7/hdfs/data<span class=nt>&lt;/value&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nt>&lt;/property&gt;</span>
</span></span><span class=line><span class=cl><span class=nt>&lt;/configuration&gt;</span>
</span></span></code></pre></td></tr></table></div></div><ol start=4><li>$HADOOP_HOME/etc/hadoop/slaves</li></ol><p>注意，只有这里是要写<strong>用户名@主机名</strong>的，否则namenode在登录datanode的时候，用的是namenode当前的用户名，造成出错</p><p>注意，slaves文件只存datanode的host，当然鉴于namenode都不干啥事，建议把master主机也作为slaver一起执行计算任务，符合社会主义核心价值观（雾）</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-txt data-lang=txt><span class=line><span class=cl>mark@mark-pc
</span></span><span class=line><span class=cl>nil@nil-PC
</span></span></code></pre></td></tr></table></div></div><h3 id=验证hadoop集群安装成功>验证hadoop集群安装成功</h3><ol><li>格式化文件系统 <code>hdfs namenode -format</code>，看到最后几行有ExitCode=0这样的字眼就说明格式化成功了，如果有误，可以检查一下xml文件的配置</li><li>在$HADOOP_HOME/sbin下运行start-all.sh，唤起namenode和datanode，分别在master和slaver上执行<code>jps</code>，如果看到以下内容就说明成功了</li></ol><p>master上看到：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-txt data-lang=txt><span class=line><span class=cl>xxxx Jps
</span></span><span class=line><span class=cl>xxxx SecondaryNameNode
</span></span><span class=line><span class=cl>xxxx NameNode
</span></span><span class=line><span class=cl>xxxx DataNode
</span></span><span class=line><span class=cl>xxxx NodeManager
</span></span><span class=line><span class=cl>xxxx ResourceManager (似乎这个是可有可无)
</span></span></code></pre></td></tr></table></div></div><p>slaver上看到</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-txt data-lang=txt><span class=line><span class=cl>xxxx Jps
</span></span><span class=line><span class=cl>xxxx DataNode
</span></span><span class=line><span class=cl>xxxx NodeManager
</span></span></code></pre></td></tr></table></div></div><p>这样就说明hadoop集群运行成功了</p><p>Notice：如果在start-all的时候发现datanode没有被唤起，大概率是datanode的id和master上的id不同，因为你执行format命令多次。解决方案是在所有的机器上都删除$HADOOP_HOME/hdfs/data/current文件夹，重新format即可。</p><h3 id=验证hadoop集群运行状态>验证hadoop集群运行状态</h3><ol><li>访问 <code>http://192.168.1.103:50070</code> (这里的ip是master的ip地址)，看到DataNodes数量符合</li><li>在master上运行<code>hdfs dfs -mkdir /hadoop</code>后，可以在网页的文件管理页面上找到新建的目录，就大功告成了！</li></ol><h2 id=安装spark>安装spark</h2><ol><li>先装一个scala，配置好SCALA_HOME，运行<code>scala</code>看到repl即可</li></ol><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=nb>export</span> <span class=nv>SCALA_HOME</span><span class=o>=</span>/usr/lib/scala-2.13.3
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>PATH</span><span class=o>=</span><span class=nv>$PATH</span>:<span class=nv>$SCALA_HOME</span>/bin
</span></span></code></pre></td></tr></table></div></div><ol start=2><li>将spark一样的解压到/usr/lib下，配置SPARK_HOME</li></ol><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=nb>export</span> <span class=nv>SPARK_HOME</span><span class=o>=</span>/usr/lib/spark-3.0.1-bin-hadoop2.7
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>PATH</span><span class=o>=</span><span class=nv>$PATH</span>:<span class=nv>$SPARK_HOME</span>/bin
</span></span></code></pre></td></tr></table></div></div><ol start=3><li>$SPARK_HOME/conf/slaves</li></ol><p>和$HADOOP_HOME/etc/hadoop/slaves内容是一样的</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>mark@mark-pc
</span></span><span class=line><span class=cl>nil@nil-PC
</span></span></code></pre></td></tr></table></div></div><ol start=4><li><p>**先启动hadoop!**启动Spark，在SPARK_HOME的sbin目录下执行start-all.sh，会唤起所有的节点，此时master中运行jps在原有基础上新增了Master，slaver里新增了Worker</p></li><li><p>进入<code>http://192.168.1.103:8080</code>看到spark监控网页，并且slaver数量正常，就说明正常启动</p></li></ol><h2 id=启动集群运行命令>启动集群，运行命令</h2><ol><li><p>运行代码<code>$SPARK_HOME/bin/run-example SparkPi 10</code>，能够正确执行计算出$\pi$，说明本地运行是ok的</p></li><li><p>向集群提交任务，执行</p></li></ol><p><code>./spark-submit --class org.apache.spark.examples.JavaSparkPi --master spark://mark-pc:7077 ../examples/jars/spark-examples_2.12-3.0.1.jar</code></p><p>在8088端口看到application和运行的机器，那就大功告成啦</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2022-10-28</span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span></span></div></div></div><div class=post-info-more><section class=post-tags></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/2022/10/git/ class=prev rel=prev title=Git使用教程><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>Git使用教程</a>
<a href=/2022/10/hugo/ class=next rel=next title=Hugo个人博客搭建>Hugo个人博客搭建<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.104.3">Hugo</a> | Theme - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden=true></i> LoveIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2020 - 2022</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://github.com/NIL-zhuang target=_blank>Nil Zhuang</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:50},comment:{},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",maxResultLength:10,noResultsFound:"No results found",snippetLength:50}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>